{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Stylometry with John Burrows’ Delta Method using Python</h2></center>\n",
    "\n",
    "__Stylometry__ is the quantitative study of literary style through computational distant reading methods. It is based on the observation that authors tend to write in relatively consistent, recognizable and unique ways. For example:\n",
    "\n",
    "* Each person has their own unique vocabulary, sometimes rich, sometimes limited. Although a larger vocabulary is usually associated with literary quality, this is not always the case.\n",
    "* Some people write in short sentences, while others prefer long blocks of text consisting of many clauses.\n",
    "* No two people use semicolons, em-dashes, and other forms of punctuation in the exact same way.\n",
    "\n",
    "However, one of the most common applications of stylometry is in authorship attribution. Given an anonymous text, it is sometimes possible to guess who wrote it by measuring certain features, like the average number of words per sentence or the propensity of the author to use “while” instead of “whilst”, and comparing the measurements with other texts written by the suspected author. This is what we will be doing in this lesson. Here, we will apply __John Burrows’ Delta Method__ to infer authorship of an anonymous text or set of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Here, we will conduct our experiment with a corpus that contains 1000 tweets and the number of unique user was 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/TDS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>profession</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@TomCruise</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Thank you to all the fans who came out to Hall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@TomCruise</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Thank you to everyone who supported Mission: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TomCruise</td>\n",
       "      <td>Actor</td>\n",
       "      <td>2,000 feet, 2,000 people, 4 hours of hiking. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@TomCruise</td>\n",
       "      <td>Actor</td>\n",
       "      <td>It was an honor to be recognized at #CinemaCon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TomCruise</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Thank you to the amazing people of New Zealand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user profession                                              tweet\n",
       "0  @TomCruise      Actor  Thank you to all the fans who came out to Hall...\n",
       "1  @TomCruise      Actor  Thank you to everyone who supported Mission: I...\n",
       "2  @TomCruise      Actor  2,000 feet, 2,000 people, 4 hours of hiking. T...\n",
       "3  @TomCruise      Actor  It was an honor to be recognized at #CinemaCon...\n",
       "4  @TomCruise      Actor  Thank you to the amazing people of New Zealand..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.tweet\n",
    "y = data.user\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.2)\n",
    "\n",
    "data_train = pd.DataFrame()\n",
    "data_train[\"tweet\"] = X_train\n",
    "data_train[\"user\"] = y_train\n",
    "\n",
    "data_test = pd.DataFrame()\n",
    "data_test[\"tweet\"] = X_test\n",
    "data_test[\"user\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/silicon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "download('punkt')\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    preprocessed_text = [token for token in tokens if any(c.isalpha() for c in token)]\n",
    "    return preprocessed_text\n",
    "\n",
    "data_train['tweet'] = data_train['tweet'].apply(preprocess)\n",
    "\n",
    "data_train = pd.DataFrame(data_train['tweet'].groupby(data_train['user']).sum()).reset_index()\n",
    "data_test = pd.DataFrame(data_test['tweet'].groupby(data_test['user']).sum()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpus = []\n",
    "for i in data_train.tweet:\n",
    "    whole_corpus += i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(30))\n",
    "#whole_corpus_freq_dist[ :30 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [word for word, freq in whole_corpus_freq_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating features for each subcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_freqs = {}\n",
    "\n",
    "for i, row in data_train.iterrows():\n",
    "    \n",
    "    feature_freqs[row.user] = {}\n",
    "    \n",
    "    overall = len(row.tweet)\n",
    "    \n",
    "    for feature in features:\n",
    "        presence = row.tweet.count(feature)\n",
    "        feature_freqs[row.user][feature] = presence / overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating feature averages and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# The data structure into which we will be storing the \"corpus standard\" statistics\n",
    "corpus_features = {}\n",
    "\n",
    "# For each feature...\n",
    "for feature in features:\n",
    "    # Create a sub-dictionary that will contain the feature's mean \n",
    "    # and standard deviation\n",
    "    corpus_features[feature] = {}\n",
    "    \n",
    "    # Calculate the mean of the frequencies expressed in the subcorpora\n",
    "    feature_average = 0\n",
    "    for i, row in data_train.iterrows():\n",
    "        feature_average += feature_freqs[row.user][feature]\n",
    "    feature_average /= len(data_train.user)\n",
    "    corpus_features[feature][\"Mean\"] = feature_average\n",
    "    \n",
    "    # Calculate the standard deviation using the basic formula for a sample\n",
    "    feature_stdev = 0\n",
    "    for i, row in data_train.iterrows():\n",
    "        diff = feature_freqs[row.user][feature] - corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev += diff*diff\n",
    "    feature_stdev /= (len(data_train.user) - 1)\n",
    "    feature_stdev = math.sqrt(feature_stdev)\n",
    "    corpus_features[feature][\"StdDev\"] = feature_stdev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Equation for the z-score statistic:__\n",
    "$$ Z_i = \\frac{C_i - \\mu_i}{\\sigma_i}$$\n",
    "This is the equation of z-score for feature $i$, where $C_i$ represents the observed frequency, the greek letter $\\mu$ represents the mean of means, and the greek letter $\\sigma$, the standard deviation.\n",
    "\n",
    "__Equation for John Burrows’ Delta statistic:__\n",
    "$$ \\Delta_c = \\sum_i \\frac{\\mid Z_c(i) - Z_t(i) \\mid}{n} $$\n",
    "where, $Z_c(i)$ is the z-score for feature $i$ in author $c$, and $Z_t(i)$ is the z-score for feature $i$ in the test case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_zscores = {}\n",
    "\n",
    "for i, row in data_train.iterrows():\n",
    "    feature_zscores[row.user] = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        # Z-score definition = (value - mean) / stddev\n",
    "        feature_val = feature_freqs[row.user][feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        feature_zscores[row.user][feature] = ((feature_val-feature_mean) / feature_stdev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating features and z-scores for test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_case):\n",
    "    # Tokenize the test case\n",
    "    testcase_tokens = nltk.word_tokenize(test_case)\n",
    "\n",
    "    # Filter out punctuation and lowercase the tokens\n",
    "    testcase_tokens = [token.lower() for token in testcase_tokens if any(c.isalpha() for c in token)]\n",
    "    \n",
    "\n",
    "    # Calculate the test case's features\n",
    "    overall = len(testcase_tokens)\n",
    "    testcase_freqs = {}\n",
    "    for feature in features:\n",
    "        presence = testcase_tokens.count(feature)\n",
    "        testcase_freqs[feature] = presence / overall\n",
    "\n",
    "    # Calculate the test case's feature z-scores\n",
    "    testcase_zscores = {}\n",
    "    for feature in features:\n",
    "        feature_val = testcase_freqs[feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev\n",
    "\n",
    "    ## Calculating Delta\n",
    "    test_result = {}\n",
    "    for i, row in data_train.iterrows():\n",
    "        delta = 0\n",
    "        for feature in features:\n",
    "            delta += math.fabs((testcase_zscores[feature] - feature_zscores[row.user][feature]))\n",
    "        delta /= len(features)\n",
    "        test_result[row.user] = delta\n",
    "\n",
    "    # min_val = sorted(list(test_result.values()))[:2]\n",
    "    # author = [k for k, v in test_result.items() if v == min_val[0] or v == min_val[1]]\n",
    "    \n",
    "    min_val = min(test_result.values())\n",
    "    author = [k for k, v in test_result.items() if v == min_val]\n",
    "    return(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.user = data_test.user.astype(str)\n",
    "result = []\n",
    "for test_case in data_test['tweet']:\n",
    "    result.append(testing(test_case)[0])\n",
    "\n",
    "data_test[\"result\"] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  90.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (data_test[data_test.user == data_test.result].shape[0]) / (data.user.nunique())\n",
    "print(\"accuracy: \", accuracy*100,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
